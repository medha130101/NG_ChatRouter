{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7c6c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4221d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3cde694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\Desktop\\NG_ChatRouter\\myvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d0d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad632f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"distilbert-base-uncased\")\n",
    "EPOCHS = int(os.getenv(\"EPOCHS\", 3))\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", 16))\n",
    "MAX_LENGTH = int(os.getenv(\"MAX_LENGTH\", 128))\n",
    "MODEL_DIR = os.getenv(\"MODEL_DIR\", \"saved_model\")\n",
    "metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f55733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path):\n",
    "    df = df = pd.read_csv(csv_path, quotechar='\"', escapechar='\\\\')\n",
    "    assert \"text\" in df.columns and \"intent\" in df.columns and \"department\" in df.columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e61594ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(df):\n",
    "    intents = sorted(df['intent'].unique().tolist())\n",
    "    intent2id = {lab:i for i,lab in enumerate(intents)}\n",
    "    id2intent = {v:k for k,v in intent2id.items()}\n",
    "    print(intents)\n",
    "    print(intent2id)\n",
    "    print(id2intent)\n",
    "    print(df.head())\n",
    "    df[\"label\"] = df[\"intent\"].map(intent2id)\n",
    "    # Note - Stratified split may fail if some classes have very few samples.\n",
    "    # train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "    try:\n",
    "        train_df, test_df = train_test_split(\n",
    "            df, test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(\"Stratified split failed:\", e)\n",
    "    print(\"Falling back to random split (no stratification).\")\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]])\n",
    "    test_ds = Dataset.from_pandas(test_df[[\"text\",\"label\"]])\n",
    "    return train_ds, test_ds, intent2id, id2intent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f7d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d610afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits,labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    f1 = metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    acc = (preds == labels).mean()\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b229213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "def train(csv_path, output_dir=MODEL_DIR):\n",
    "    df = load_data(csv_path)\n",
    "    train_ds, test_ds, intent2id, id2intent = prepare_datasets(df)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    train_ds = train_ds.map(lambda examples: tokenize_function(examples, tokenizer), batched=True)\n",
    "    test_ds = test_ds.map(lambda examples: tokenize_function(examples, tokenizer), batched=True)\n",
    "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    test_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(intent2id))\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    ")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    import json\n",
    "    with open(os.path.join(output_dir, \"intent2id.json\"), \"w\") as f:\n",
    "        json.dump(intent2id, f)\n",
    "    with open(os.path.join(output_dir, \"id2intent.json\"), \"w\") as f:\n",
    "        json.dump(id2intent, f)\n",
    "    print(\"Training complete. Model saved to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd77d93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['complaint', 'faq', 'general', 'profile_change', 'transaction_query']\n",
      "{'complaint': 0, 'faq': 1, 'general': 2, 'profile_change': 3, 'transaction_query': 4}\n",
      "{0: 'complaint', 1: 'faq', 2: 'general', 3: 'profile_change', 4: 'transaction_query'}\n",
      "                                     text          intent        department\n",
      "0       I want to change my email address  profile_change  account_services\n",
      "1  My card was charged twice, need refund       complaint           billing\n",
      "2              How do I reset my password             faq  account_services\n",
      "3     I'd like to update my KYC documents  profile_change  account_services\n",
      "4  I want to dispute a transaction of $50       complaint           billing\n",
      "Stratified split failed: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Falling back to random split (no stratification).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\Desktop\\NG_ChatRouter\\myvenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 447.36 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 655.92 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\Dell\\Desktop\\NG_ChatRouter\\myvenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "100%|██████████| 3/3 [00:07<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 7.452, 'train_samples_per_second': 3.221, 'train_steps_per_second': 0.403, 'train_loss': 1.574869155883789, 'epoch': 3.0}\n",
      "Training complete. Model saved to saved_model\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    import sys\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--csv\", type=str, default=\"Data/example_data.csv\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"saved_model\")\n",
    "\n",
    "    # ignore unrecognized args (like --f)\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    train(args.csv, args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09bbba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "204a50f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTING_TABLE  = {\n",
    "    \"profile_change\": \"account_services\",\n",
    "    \"complaint\":\"billing\",\n",
    "    \"faq\":\"customer_support\",\n",
    "    \"transation_query\":\"transactions\",\n",
    "    \"general\":\"customer_support\",\n",
    "    \"loans\":\"loans_officer\"        \n",
    "}\n",
    "\n",
    "def rule_based_override(metadata):\n",
    "    if(metadata.get(\"is_vip\")):\n",
    "        return \"priority_support\"\n",
    "    if(metadata.get(\"customer_tier\") == \"gold\"):\n",
    "        return \"priority_support\"\n",
    "    return None\n",
    "\n",
    "def map_intent_to_department(intent_label):\n",
    "    return ROUTING_TABLE.get(intent_label, \"customer_support\")\n",
    "\n",
    "def decide_route(predicted_intent, confidence, metadata):\n",
    "    override = rule_based_override(metadata)\n",
    "    if override:\n",
    "        logger.info(f\"Rule override to {override} based on metadata {metadata}\")\n",
    "        return override, \"rule_override\"\n",
    "    dept = map_intent_to_department(predicted_intent)\n",
    "    return dept, \"intent_mapping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "807f8efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Running Routing Tests ====================\n",
      "\n",
      "Text: I want to change my email address\n",
      " → Predicted Intent: faq\n",
      " → Confidence: 0.224\n",
      " → Clarification Needed: True\n",
      " → Department: None\n",
      " → Reason: low_confidence_clarification\n",
      "------------------------------------------------------------\n",
      "Text: My card was charged twice\n",
      " → Predicted Intent: faq\n",
      " → Confidence: 0.226\n",
      " → Clarification Needed: True\n",
      " → Department: None\n",
      " → Reason: low_confidence_clarification\n",
      "------------------------------------------------------------\n",
      "Text: How to apply for a loan?\n",
      " → Predicted Intent: faq\n",
      " → Confidence: 0.226\n",
      " → Clarification Needed: True\n",
      " → Department: None\n",
      " → Reason: low_confidence_clarification\n",
      "------------------------------------------------------------\n",
      "Text: Please reset my password\n",
      " → Predicted Intent: faq\n",
      " → Confidence: 0.222\n",
      " → Clarification Needed: True\n",
      " → Department: None\n",
      " → Reason: low_confidence_clarification\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test_inference.py\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# === Config ===\n",
    "MODEL_DIR = \"saved_model\"\n",
    "CONFIDENCE_THRESHOLD = 0.7\n",
    "\n",
    "# === Load model, tokenizer, and label maps ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, \"id2intent.json\"), \"r\") as f:\n",
    "    id2intent = json.load(f)\n",
    "\n",
    "# === Helper: predict intent & confidence ===\n",
    "def predict_intent(text):\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=-1).squeeze().cpu().numpy()\n",
    "    top_idx = int(probs.argmax())\n",
    "    confidence = float(probs[top_idx])\n",
    "    intent_label = id2intent.get(str(top_idx), id2intent.get(top_idx, \"unknown\"))\n",
    "    return intent_label, confidence, probs\n",
    "\n",
    "# === Test examples ===\n",
    "examples = [\n",
    "    {\"text\": \"I want to change my email address\", \"metadata\": {}},\n",
    "    {\"text\": \"My card was charged twice\", \"metadata\": {\"is_vip\": False}},\n",
    "    {\"text\": \"How to apply for a loan?\", \"metadata\": {\"customer_tier\": \"gold\"}},\n",
    "    {\"text\": \"Please reset my password\", \"metadata\": {}},\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*20} Running Routing Tests {'='*20}\\n\")\n",
    "for ex in examples:\n",
    "    intent, confidence, probs = predict_intent(ex[\"text\"])\n",
    "    clarification_needed = confidence < CONFIDENCE_THRESHOLD\n",
    "    if clarification_needed:\n",
    "        dept = None\n",
    "        decision_reason = \"low_confidence_clarification\"\n",
    "    else:\n",
    "        dept, decision_reason = decide_route(intent, confidence, ex[\"metadata\"])\n",
    "\n",
    "    print(f\"Text: {ex['text']}\")\n",
    "    print(f\" → Predicted Intent: {intent}\")\n",
    "    print(f\" → Confidence: {confidence:.3f}\")\n",
    "    print(f\" → Clarification Needed: {clarification_needed}\")\n",
    "    print(f\" → Department: {dept}\")\n",
    "    print(f\" → Reason: {decision_reason}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550bea08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
